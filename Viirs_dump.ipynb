{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Viirs_dump.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNqGvwFjUDcJRujmCCG1VVW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athapa42/VIIRS/blob/master/Viirs_dump.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYM01t0D4gLJ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Module: read_tropomi_no2_and_dump_ascii.py**\n",
        "\n",
        "**Disclaimer**: The code is for demonstration purposes only. Users are responsible to check for accuracy and revise to fit their objective.\n",
        "\n",
        "**Author**: Justin Roberts-Pierel and Pawan Gupta, 2015.\n",
        "\n",
        "**Modified to work with TROPOMI** : Vikalp Mishra, 2019 \n",
        "\n",
        "**Organization**: NASA ARSET\n",
        "\n",
        "**Modified to work with VIIRS data**: Aavash Thapa, 2020\n",
        "\n",
        "**Purpose**: To save data into a csv file from a VIIRS Deep Blue netcdf file\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkGlEu0bnbJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTxkRB897HJ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "ab64c002-08fa-4ffc-b2c5-a44a4954af49"
      },
      "source": [
        "pip install netCDF4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting netCDF4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/4f/d49fe0c65dea4d2ebfdc602d3e3d2a45a172255c151f4497c43f6d94a5f6/netCDF4-1.5.3-cp36-cp36m-manylinux1_x86_64.whl (4.1MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1MB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from netCDF4) (1.18.5)\n",
            "Collecting cftime\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/f4/31cb9b65f462ea960bd334c5466313cb7b8af792f272546b68b7868fccd4/cftime-1.2.1-cp36-cp36m-manylinux1_x86_64.whl (287kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 29.2MB/s \n",
            "\u001b[?25hInstalling collected packages: cftime, netCDF4\n",
            "Successfully installed cftime-1.2.1 netCDF4-1.5.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wpy4FnpQvFtI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "4c654bc4-c45f-48cb-d7f6-5ce5455ca68d"
      },
      "source": [
        "\n",
        "#!/usr/bin/python      \n",
        "from netCDF4 import Dataset\n",
        "import numpy as np\n",
        "import sys\n",
        "import time\n",
        "import calendar\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "from shutil import copyfile\n",
        "\n",
        "\n",
        "#This finds the user's current path so that all hdf4 files can be found\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    fileList = open('/content/drive/My Drive/Colab Notebooks/VIIRS/fileList.txt', 'r')\n",
        "\n",
        "except:\n",
        "    print('Did not find a text file containing file names (perhaps name does not match)')\n",
        "    sys.exit()\n",
        "\n",
        "#loops through all files listed in the text file\n",
        "for FILE_NAME in fileList:\n",
        "    FILE_NAME=FILE_NAME.strip()\n",
        "    user_input=input('\\nWould you like to process\\n' + FILE_NAME + '\\n\\n(Y/N)')\n",
        "    if (user_input == 'N' or user_input == 'n'):\n",
        "        print('Skipping...')\n",
        "        continue\n",
        "    else:\n",
        "        file = Dataset('/content/drive/My Drive/Colab Notebooks/VIIRS/'+ FILE_NAME, 'r')\n",
        "     #   grp='PRODUCT' \n",
        "# read the data\n",
        "        if 'AERDB' in FILE_NAME:\n",
        "            print('This is a VIIRS Deep Blue file.')\n",
        "            #this is how you access the data tree in an hdf5 file\n",
        "            SDS_NAME='Aerosol_Optical_Thickness_550_Land_Best_Estimate'    \n",
        "        ds=file\n",
        "       # grp='PRODUCT'  \n",
        "        lat= ds.variables['Latitude'][:][:]\n",
        "        lon= ds.variables['Longitude'][:][:]\n",
        "        data= ds.variables[SDS_NAME]\n",
        "\n",
        "        #get necessary attributes \n",
        "        fv=data._FillValue\n",
        "          \n",
        "        fileparts=FILE_NAME.split('.')\n",
        "\n",
        "        #There are some columns that are going to be the same\n",
        "        #like the year, month and so on listed below.\n",
        "        #Therefore, we can make the columns for them to store\n",
        "        #the data for every row.\n",
        "        year = np.zeros(lat.shape)\n",
        "        mth = np.zeros(lat.shape)\n",
        "        doy = np.zeros(lat.shape)\n",
        "        hr = np.zeros(lat.shape)\n",
        "        mn = np.zeros(lat.shape)\n",
        "        \n",
        "        for i in range(0,lat.shape[0]):\n",
        "            y= fileparts[1][1:5]\n",
        "            h = fileparts[2][0:2]\n",
        "            m = fileparts[2][2:4]\n",
        "            date = y + ',' + fileparts[1][5:8] + ',' + h + ',' + m\n",
        "            t2 = dt.datetime.strptime(date,'%Y,%j,%H,%M')\n",
        "           \n",
        "            mt = t2.month\n",
        "            d = t2.day\n",
        "            \n",
        "            year[i][:] = y\n",
        "            mth[i][:] = mt\n",
        "            doy[i][:] = d\n",
        "            hr[i][:] = h\n",
        "            mn[i][:] = m\n",
        "            \n",
        "        vlist = list(file.variables.keys())\n",
        "        #print('vlist: ', vlist)\n",
        "        \n",
        "        #create the dataframe and enter the values here\n",
        "        df = pd.DataFrame()\n",
        "        df['Year'] = year.ravel()\n",
        "        df['Month'] = mth.ravel()\n",
        "        df['Day'] = doy.ravel()\n",
        "        df['Hour'] = hr.ravel()\n",
        "        df['Minute'] = mn.ravel()\n",
        "        \n",
        "        #0-->Aerosol_Optical_Thickness_550_Land\n",
        "        #3-->Aerosol_Optical_Thickness_550_Land_Ocean_Best_Estimate\n",
        "        #8-->Aerosol_Optical_Thickness_QA_Flag_Land\n",
        "        #11-->Aerosol_Type_Land_Ocean\n",
        "        #18-->Angstrom_Exponent_Land_Ocean_Best_Estimate\n",
        "        sds_lst = [ 'Aerosol_Optical_Thickness_550_Land',\n",
        "                   'Aerosol_Optical_Thickness_550_Land_Ocean_Best_Estimate',\n",
        "                   'Aerosol_Optical_Thickness_QA_Flag_Land',\n",
        "                   'Aerosol_Type_Land_Ocean',\n",
        "                   'Angstrom_Exponent_Land_Ocean_Best_Estimate']\n",
        "        \n",
        "        #This for loop saves all of the SDS in the dictionary at the top (dependent on file type) to the array (with titles)\n",
        "        #All the sds that we need seem to be contained in this range.\n",
        "        #Can extend this range to loop through more sds variables in the NC file.\n",
        "        for i in range(0,20):\n",
        "            SDS_NAME=vlist[(i)] # The name of the sds to read\n",
        "            \n",
        "            if SDS_NAME in sds_lst:\n",
        "                print('SDS_NAME', SDS_NAME)\n",
        "                #get current SDS data, or exit program if the SDS is not found in the file\n",
        "                #try:\n",
        "                sds=ds.variables[SDS_NAME]\n",
        "               \n",
        "                #for i in range(0, len(sds)):\n",
        "                #if len(sds.shape) == 3:\n",
        "                #print(SDS_NAME,sds.shape)\n",
        "                #get attributes for current SDS\n",
        "                if 'qa' in SDS_NAME:\n",
        "                    scale=sds.scale_factor\n",
        "                else: scale = 1.0\n",
        "            \n",
        "                fv=sds._FillValue\n",
        "    \n",
        "            #get SDS data as a vector\n",
        "                data=sds[:].ravel()\n",
        "                #print(data)\n",
        "               #The next few lines change fill value/missing value to NaN so that we can multiply valid values by the scale factor, then back to fill values for saving\n",
        "                data=data.astype(float)\n",
        "                data=(data)*scale  \n",
        "                data[np.isnan(data)]=fv\n",
        "                data[data==float(fv)]=np.nan\n",
        "                data=np.array(data[:])\n",
        "                df[SDS_NAME] = data\n",
        "    \n",
        "    outfilename=FILE_NAME[:-3]+'.csv'    \n",
        "    df.to_csv(outfilename, index = False)\n",
        "    copyfile(outfilename, \"drive/My Drive/Colab Notebooks/VIIRS/\" + outfilename)    \n",
        "    print('\\nAll files have been saved successfully.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "\n",
            "Would you like to process\n",
            "AERDB_L2_VIIRS_SNPP.A2020056.1954.001.2020057113600.nc\n",
            "\n",
            "(Y/N)y\n",
            "This is a VIIRS Deep Blue file.\n",
            "SDS_NAME Aerosol_Optical_Thickness_550_Land\n",
            "SDS_NAME Aerosol_Optical_Thickness_550_Land_Ocean_Best_Estimate\n",
            "SDS_NAME Aerosol_Optical_Thickness_QA_Flag_Land\n",
            "SDS_NAME Aerosol_Type_Land_Ocean\n",
            "SDS_NAME Angstrom_Exponent_Land_Ocean_Best_Estimate\n",
            "\n",
            "All files have been saved successfully.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}